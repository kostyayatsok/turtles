{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- single head\n",
        "- sgd\n",
        "- all classes"
      ],
      "metadata": {
        "id": "-eWtjzNCda_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMhZ1PFOEG5J",
        "outputId": "5379e0b2-01b0-4b7e-e589-80d0650f374d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'turtles'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
            "remote: Total 285 (delta 146), reused 196 (delta 64), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (285/285), 189.26 MiB | 21.36 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "Checking out files: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://kostyayatsok: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@github.com/kostyayatsok/turtles.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuWeBlKzE7zG",
        "outputId": "6b5c1db9-bcf2-4af3-de8f-0c176d1e422e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/turtles\n"
          ]
        }
      ],
      "source": [
        "%cd turtles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzIEVh-3NWs8",
        "outputId": "51950a2f-ae35-463a-e237-a48f4da30d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEAD is now at acf0546 fix train_view mode\n"
          ]
        }
      ],
      "source": [
        "!git reset --hard $HEAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpb93Ya4IkCD",
        "outputId": "e12764da-3017-4a73-a7b1-e6b4d5dfeb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/kostyayatsok/turtles\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   454481d..aa59d5b  main       -> origin/main\n",
            "Auto-merging submission.py\n",
            "CONFLICT (content): Merge conflict in submission.py\n",
            "Automatic merge failed; fix conflicts and then commit the result.\n"
          ]
        }
      ],
      "source": [
        "!git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN2yC27tTh16",
        "outputId": "e78e2827-53f0-4b5c-be80-0056d3faffef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 12.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U38XgEiE9cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cdc9b70-6657-4867-86e2-d559becba3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n",
            "Using (1501, 4) images for training and (644, 4) images for validation\n",
            "--2022-03-20 09:58:03--  https://storage.googleapis.com/dm-turtle-recall/images.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.217.128, 173.194.218.128, 108.177.11.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.217.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6481244160 (6.0G) [application/x-tar]\n",
            "Saving to: ‘./turtle_recall/images/images.tar’\n",
            "\n",
            "./turtle_recall/ima 100%[===================>]   6.04G   147MB/s    in 56s     \n",
            "\n",
            "2022-03-20 09:59:00 (110 MB/s) - ‘./turtle_recall/images/images.tar’ saved [6481244160/6481244160]\n",
            "\n",
            "The total number of images is: 13891\n",
            "(1501, 4) (644, 4) (490, 2)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 128MB/s]\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --mode train_views"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlvWQyAnAHX8",
        "outputId": "3330e47f-99a6-48f6-ca04-fa68c553c2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n",
            "Using (1501, 4) images for training and (644, 4) images for validation\n",
            "(1501, 4) (644, 4) (490, 2)\n",
            "The total number of images is: 13891\n",
            "Epoch 0/1\n",
            "----------\n",
            "100% 188/188 [00:48<00:00,  3.90it/s]\n",
            "train Loss: 0.3631 Acc: 0.8787\n",
            "100% 81/81 [00:19<00:00,  4.06it/s]\n",
            "val Loss: 0.2261 Acc: 0.9394\n",
            "\n",
            "Epoch 1/1\n",
            "----------\n",
            "100% 188/188 [00:50<00:00,  3.69it/s]\n",
            "train Loss: 0.1686 Acc: 0.9500\n",
            "100% 81/81 [00:23<00:00,  3.42it/s]\n",
            "val Loss: 0.2850 Acc: 0.9394\n",
            "\n",
            "Training complete in 2m 23s\n",
            "Best val Acc: 0.939441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py \\\n",
        "    --mode train \\\n",
        "    --use_extra_data \\\n",
        "    --new_turtles_fraq 0.05 \\\n",
        "    --checkpoint checkpoints/model.pt\n",
        "    # --use_extra_ids \\\n",
        "    # --views_model checkpoints/model_views.pt \\\n",
        "    # --wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fogQk_2NP3wO",
        "outputId": "3d90cf40-ac45-42b6-be13-92095496b0c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n",
            "The total number of images is: 13891\n",
            "Add 107 new_turtles\n",
            "Using (2023, 4) images for training and (868, 4) images for validation\n",
            "(2023, 4) (868, 4) (490, 2)\n",
            "Epoch 0/9\n",
            "----------\n",
            "100% 253/253 [00:56<00:00,  4.45it/s]\n",
            "train Loss: 0.5411 Acc: 0.8952\n",
            "100% 109/109 [00:23<00:00,  4.55it/s]\n",
            "val Loss: 1.3477 Acc: 0.6763\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "100% 253/253 [00:58<00:00,  4.31it/s]\n",
            "train Loss: 0.4108 Acc: 0.9268\n",
            "100% 109/109 [00:23<00:00,  4.55it/s]\n",
            "val Loss: 1.2409 Acc: 0.6970\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.43it/s]\n",
            "train Loss: 0.3128 Acc: 0.9506\n",
            "100% 109/109 [00:24<00:00,  4.50it/s]\n",
            "val Loss: 1.2417 Acc: 0.7074\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "100% 253/253 [00:56<00:00,  4.45it/s]\n",
            "train Loss: 0.2240 Acc: 0.9718\n",
            "100% 109/109 [00:24<00:00,  4.39it/s]\n",
            "val Loss: 1.1490 Acc: 0.7304\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.39it/s]\n",
            "train Loss: 0.1540 Acc: 0.9847\n",
            "100% 109/109 [00:24<00:00,  4.46it/s]\n",
            "val Loss: 1.1477 Acc: 0.7212\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.36it/s]\n",
            "train Loss: 0.1199 Acc: 0.9876\n",
            "100% 109/109 [00:24<00:00,  4.46it/s]\n",
            "val Loss: 1.1075 Acc: 0.7512\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.39it/s]\n",
            "train Loss: 0.1000 Acc: 0.9891\n",
            "100% 109/109 [00:24<00:00,  4.49it/s]\n",
            "val Loss: 1.1361 Acc: 0.7281\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.42it/s]\n",
            "train Loss: 0.0650 Acc: 0.9970\n",
            "100% 109/109 [00:24<00:00,  4.50it/s]\n",
            "val Loss: 1.0933 Acc: 0.7500\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.40it/s]\n",
            "train Loss: 0.0549 Acc: 0.9990\n",
            "100% 109/109 [00:24<00:00,  4.43it/s]\n",
            "val Loss: 1.1086 Acc: 0.7546\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "100% 253/253 [00:57<00:00,  4.40it/s]\n",
            "train Loss: 0.0520 Acc: 0.9965\n",
            "100% 109/109 [00:24<00:00,  4.46it/s]\n",
            "val Loss: 1.0871 Acc: 0.7523\n",
            "\n",
            "Training complete in 13m 42s\n",
            "Best val Acc: 0.754608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tYLxY752eDvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 submission.py \\\n",
        "    --checkpoint checkpoints/model.pt \\\n",
        "    # --views_model checkpoints/model_views.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we2NnhJ76qTU",
        "outputId": "50b4af5c-d002-46b6-8531-c0366534e899"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n",
            "The total number of images is: 13891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "ZH2hb3T7UhVy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git config --global user.email \"kielenik@edu.hse.ru\"\n",
        "! git config --global user.name \"kostyayatsok\""
      ],
      "metadata": {
        "id": "nrIyHNtA3H6e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"simple model good results. changes:\\n-adam->sgd\\n-no padding\\n-random crops\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTUi48ehVoOR",
        "outputId": "e80dc1f8-91f6-4c15-d073-6fee5fd3c3dd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 4adc889] simple model good results. changes:\\n-adam->sgd\\n-no padding\\n-random crops\n",
            " 5 files changed, 503 insertions(+), 497 deletions(-)\n",
            " rewrite submission_turtles.csv (97%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4VdvFGFUjsm",
        "outputId": "d3ac9a66-29be-4058-c058-9cde9f9e3661"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 9, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 84.24 MiB | 8.96 MiB/s, done.\n",
            "Total 9 (delta 6), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (6/6), completed with 6 local objects.\u001b[K\n",
            "remote: warning: See http://git.io/iEPt8g for more information.\u001b[K\n",
            "remote: warning: File checkpoints/model.pt is 90.77 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: warning: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/kostyayatsok/turtles.git\n",
            "   6ba94d9..4adc889  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPGldrPGY8fB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"/content/turtles/checkpoints/improved-net.pt\")\n",
        "# # files.download(\"/content/turtles/checkpoints/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSXnRkFIm7aE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd09c1f7-0cbb-4219-fccd-b8ec3ac9c1e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n",
            "Using (1501, 4) images for training and (644, 4) images for validation\n",
            "The total number of images is: 13891\n",
            "(1501, 4) (644, 4) (490, 2)\n",
            "100% 188/188 [00:50<00:00,  3.71it/s]\n",
            "Phase train Loss: 0.0033 Acc: 1.0000 Map5: 1.0000 Map1: 1.0000\n",
            "100% 81/81 [00:22<00:00,  3.57it/s]\n",
            "Phase val Loss: 3.0426 Acc: 0.3696 Map5: 0.4466 Map1: 0.3696\n"
          ]
        }
      ],
      "source": [
        "from config import CHECKPOINTS_DIR\n",
        "!python3 train.py --mode eval --checkpoint {CHECKPOINTS_DIR}/model.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tBpFrdR3U3O"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# files.download(\"/content/turtles/checkpoints/improved-net.pt\")\n",
        "files.download(\"/content/turtles/checkpoints/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMKgOVtvykB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7997f84f-eeac-4aaa-bcca-508e35e1341d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'checkpoints/improved-net.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! cp checkpoints/improved-net.pt ./ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzjL2bdgXaXA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "9ac5bab6-aac9-430a-c40b-60f6605d12eb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3ccb902dec1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_val_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "from src.model import get_model\n",
        "from config import *\n",
        "import torch\n",
        "from src.data import load_csv, train_val_split\n",
        "\n",
        "train, test = load_csv()\n",
        "train = train.reset_index()\n",
        "train, val = train_val_split(train, 0.7)\n",
        "\n",
        "model = get_model(num_classes)\n",
        "model.load_state_dict(torch.load(\"./improved-net.pt\"))\n",
        "model.load_state_dict(torch.load(\"../model.pt\"))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# for m in model.modules():\n",
        "#     if isinstance(m, torch.nn.BatchNorm2d):\n",
        "#         print(m.running_mean)\n",
        "\n",
        "# model(torch.randn(2, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akQnbUtgkeer"
      },
      "outputs": [],
      "source": [
        "from byol_pytorch import BYOL\n",
        "\n",
        "learner = BYOL(\n",
        "        model,\n",
        "        image_size = input_size,\n",
        "        hidden_layer = 'avgpool',\n",
        "        use_momentum=False,\n",
        "    )\n",
        "# _, emb = learner(torch.randn(2, 3, 224, 224), return_embedding = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Feah5IXb4S"
      },
      "outputs": [],
      "source": [
        "learner.online_encoder.get_representation(torch.randn(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXI3EVY_lgEN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu_PAlO4pV5y"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def pad(img):\n",
        "    pad_h = max(img.size(1), img.size(2)) - img.size(1)\n",
        "    pad_w = max(img.size(1), img.size(2)) - img.size(2)\n",
        "    img = torch.nn.functional.pad(img, (0, pad_w, 0, pad_h, 0, 0))\n",
        "    img = T.Resize(256)(img)\n",
        "    return img\n",
        "train[\"emb\"] = None\n",
        "mask = train['is_known_id']\n",
        "with torch.no_grad():\n",
        "    for i, row in tqdm(train[mask].iterrows(), total=train[mask].shape[0]):\n",
        "        img = read_image(f\"turtle_recall/images/{row['image_id']}.JPG\") / 255.\n",
        "        img = pad(img)\n",
        "        emb = learner.online_encoder.get_representation(img.unsqueeze(0))\n",
        "        # print(train.loc[i])\n",
        "        train.loc[i, 'emb'] = [emb.cpu().numpy()]\n",
        "        # print(train)\n",
        "        # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgzSfWB3iyZi"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRSFJltdVDqt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "from tqdm import trange\n",
        "\n",
        "def dist(a, b):\n",
        "    return np.dot(a[0] - b[0], a[0] - b[0])\n",
        "\n",
        "new_mask = ~train.image_location.isna()\n",
        "\n",
        "tp, tn, fp, fn = 0, 0, 0, 0\n",
        "for i in trange(100000):\n",
        "    rows = train[new_mask].sample(2)\n",
        "    if rows.iloc[0]['image_location'].lower()!=rows.iloc[1]['image_location'].lower():\n",
        "        continue\n",
        "\n",
        "    d = dist(rows.iloc[0]['emb'], rows.iloc[1]['emb'])\n",
        "    \n",
        "    if rows.iloc[0]['turtle_id']!=rows.iloc[1]['turtle_id']:\n",
        "        if d < 1:\n",
        "            fp+=1\n",
        "        else:\n",
        "            tn+=1\n",
        "    else:\n",
        "        if d < 1:\n",
        "            tp+=1\n",
        "        else:\n",
        "            fn+=1\n",
        "    \n",
        "    # img1 = read_image(f\"turtle_recall/images/{rows.iloc[0]['image_id']}.JPG\")\n",
        "    # img2 = read_image(f\"turtle_recall/images/{rows.iloc[1]['image_id']}.JPG\")\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.imshow(img1.numpy().transpose((1, 2, 0)))\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.imshow(img2.numpy().transpose((1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "    # print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7M5XR_Pkizn"
      },
      "outputs": [],
      "source": [
        "tp/(tp+tn), tp/(tp+fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdOvlThunGxK"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img2.numpy().transpose((1, 2, 0)))\n",
        "plt.show()\n",
        "plt.imshow(img3.numpy().transpose((1, 2, 0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-PyE1LNnqa9"
      },
      "outputs": [],
      "source": [
        "img1=pad(img1)\n",
        "img2=pad(img2)\n",
        "img3=pad(img3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjse30G7nU9G"
      },
      "outputs": [],
      "source": [
        "_, emb = learner(torch.stack([img1, img2, img3], dim=0), return_embedding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd9Op1NioKxh"
      },
      "outputs": [],
      "source": [
        "torch.dot(emb[0]-emb[1], emb[0]-emb[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFS9TFBRnbNR"
      },
      "outputs": [],
      "source": [
        "torch.dot(emb[0]-emb[2], emb[0]-emb[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11EQoC_no78Q"
      },
      "outputs": [],
      "source": [
        "torch.dot(emb[1]-emb[2], emb[1]-emb[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5mbTlyevRCC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "answ_test = pd.read_csv(\"../submission (35).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(answ_test[\"prediction1\"], bins=101)"
      ],
      "metadata": {
        "id": "l6W1th-k_STb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.data import load_csv\n",
        "\n",
        "train, val, test = load_csv(False)"
      ],
      "metadata": {
        "id": "ZvOlTK8OziW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "groups = test.groupby(\"image_location\").groups\n",
        "for k, v in groups.items():\n",
        "    print(\"\\n\"*10, k)\n",
        "    for i, idx in enumerate(v):\n",
        "        id = test.loc[idx][\"image_id\"]\n",
        "        img = plt.imread(f\"/content/turtles/turtle_recall/images/{id}.JPG\")\n",
        "        plt.imshow(img, 'gray')\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "RLr0KGuhFufE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "-IKnivRHkZ4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhZmPNHcvZvD"
      },
      "outputs": [],
      "source": [
        "preds = train.groupby(by = \"turtle_id\").groups"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.image_id test.image_id"
      ],
      "metadata": {
        "id": "4fTL2Hc4S394"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7m5DRqGv4Gq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def adjust_contrast_brightness(img, contrast:float=1.0, brightness:int=0):\n",
        "    \"\"\"\n",
        "    Adjusts contrast and brightness of an uint8 image.\n",
        "    contrast:   (0.0,  inf) with 1.0 leaving the contrast as is\n",
        "    brightness: [-255, 255] with 0 leaving the brightness as is\n",
        "    \"\"\"\n",
        "    brightness += int(round(255*(1-contrast)/2))\n",
        "    return cv2.addWeighted(img, contrast, img, 0, brightness)\n",
        "\n",
        "for k, v in preds.items():\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    image_ids = []\n",
        "    for i, idx in enumerate(v):\n",
        "        id = train.loc[idx][\"image_id\"]\n",
        "        image_ids.append(id)\n",
        "        img = plt.imread(f\"/content/turtles/turtle_recall/images/{id}.JPG\")\n",
        "        plt.subplot(1, len(v), i+1)\n",
        "        # img = img.mean(axis=-1).astype(np.uint8)\n",
        "\n",
        "        img = adjust_contrast_brightness(img, 5., -5.)\n",
        "\n",
        "        # img = cv2.Canny(img,100,150)\n",
        "        plt.imshow(img, 'gray')\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print(image_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ScA2AHRxKbk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "turtles.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}